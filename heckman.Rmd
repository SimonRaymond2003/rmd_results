---
title: "Heckman Selection Model Analysis"
output: html_document
---

```{r options, include=FALSE}
options(max.print=10000)
```

```{r}
library(data.table)
outcome_data <- fread("predict_outcome.csv.gz")
select_data <- fread("predict_select.csv.gz")
```

```{r}
select_data[, score_diff := as.integer(score_diff > 0)]
```


```{r}
select_data[, attempt := as.numeric(as.character(attempt))]
```

```{r}
library(data.table)
library(xgboost)
library(ROCR)
library(parallel)

# Prepare the XGBoost matrices
xs <- model.matrix(~ . - 1 - attempt - my_id, data = select_data)
y <- as.numeric(as.character(select_data$attempt))


# Create parameter grid

# Create parameter grid
grid <- expand.grid(
  eta = seq(0.001, 0.03, by = 0.01),
  max_depth = seq(5, 5, by = 2),
  min_child_weight = seq(1, 1, by = 1),        
  subsample = seq(1, 1, by = 0.2),
  colsample_bytree = seq(1, 1, by = 0.2),
  lambda = seq(1, 1, by = 1),                
  alpha = seq(0, 0, by = 1),                  
  gamma = seq(0, 0, by = 0.1),               
  nrounds = seq(100, 250, by = 50)
)


# Sample grid points
conf_lev <- .95
num_max <- 5
n <- ceiling(log(1-conf_lev)/log(1-num_max/nrow(grid)))
ind <- sample(nrow(grid), n, replace = FALSE)
rgrid <- grid[ind, ]

# Set up parallel processing
nc <- detectCores() - 1

# Calculate scale_pos_weight for imbalanced dataset
scale_pos_weight <- sum(y == 0) / sum(y == 1)

# Validation phase
#cat("\nPhase 1: Validation Phase\n")
n_validations <- 3
validation_results <- matrix(nrow = nrow(rgrid), ncol = n_validations)

for (j in 1:nrow(rgrid)) {
  #cat("\nTesting parameter set", j, "of", nrow(rgrid), "\n")
  
  for (i in 1:n_validations) {
    #cat("\nValidation iteration", i, "of", n_validations, "\n")             
    # Create validation split
    idx <- unique(sample(nrow(xs), nrow(xs), TRUE))
    train_x <- xs[idx, ]
    train_y <- y[idx]
    val_x <- xs[-idx, ]
    val_y <- y[-idx]
    
    prm <- list(
      booster = "gbtree",
      objective = "binary:logistic",
      max_depth = rgrid[j, "max_depth"],
      eta = rgrid[j, "eta"],
      subsample = rgrid[j, "subsample"],
      colsample_bytree = rgrid[j, "colsample_bytree"],
      gamma = rgrid[j, "gamma"],
      min_child_weight = rgrid[j, "min_child_weight"],
      alpha = rgrid[j, "alpha"],
      lambda = rgrid[j, "lambda"],
      scale_pos_weight = scale_pos_weight,
      nthread = nc
    )
    
    dm_train <- xgb.DMatrix(data = train_x, label = train_y)
    mdl <- xgb.train(
      params = prm,
      data = dm_train,
      nrounds = rgrid[j, "nrounds"],
      verbose = FALSE
    )
    
    p <- predict(mdl, xgb.DMatrix(data = val_x))
    pred <- prediction(p, val_y)
    validation_results[j, i] <- performance(pred, "auc")@y.values[[1]]
  }
}

# Run 100 tests with best parameters
cat("\nRunning X test iterations with best parameters...\n")
test_aucs <- c()

best_params_idx <- which.max(rowMeans(validation_results))
best_params <- rgrid[best_params_idx,]

# Train final model with best parameters
best_params_list <- as.list(best_params[-which(names(best_params) == "nrounds")])
best_params_list$booster <- "gbtree"
best_params_list$objective <- "binary:logistic"
best_params_list$scale_pos_weight <- scale_pos_weight
best_params_list$nthread <- nc

for(i in 1:20) {
  # Create test split
  idx <- unique(sample(nrow(xs), nrow(xs), T))
  train_x <- xs[idx, ]
  train_y <- y[idx]
  test_x <- xs[-idx, ]
  test_y <- y[-idx]
  
  # Train model on full unbalanced dataset
  dm_train <- xgb.DMatrix(data = train_x, label = train_y)
  mdl <- xgb.train(
    params = best_params_list,
    data = dm_train,
    nrounds = best_params[["nrounds"]],
    verbose = FALSE
  )
  
  # Test on unbalanced test set
  p <- predict(mdl, xgb.DMatrix(data = test_x))
  pred <- prediction(p, test_y)
  test_aucs[i] <- performance(pred, "auc")@y.values[[1]]
}
# Print results
cat("\nTest Results over 100 iterations:\n")
cat("Mean AUC:", mean(test_aucs), "\n")
cat("SD AUC:", sd(test_aucs), "\n")
cat("95% CI:", mean(test_aucs) - 1.96 * sd(test_aucs), 
    "to", mean(test_aucs) + 1.96 * sd(test_aucs), "\n")

# Plotting AUC distribution
plot(test_aucs, col="red", pch=20,
     main="AUC Test Distribution",
     xlab="Iteration", ylab="AUC")
abline(h=mean(test_aucs), col="blue", lwd=2, lty=2)  # Add this line to show mean
abline(h=mean(test_aucs) - 1.96 * sd(test_aucs), col="green", lwd=2, lty=3)
abline(h=mean(test_aucs) + 1.96 * sd(test_aucs), col="green", lwd=2, lty=3)
legend("bottomright", 
       legend=c("AUC Values", "Mean", "95% CI Bounds"),
       col=c("red", "blue", "green"),
       lty=c(NA, 2, 3),
       pch=c(20, NA, NA))

# Now train final model on full data for z scores
dm_full <- xgb.DMatrix(data = xs, label = y)
final_model <- xgb.train(
  params = best_params_list,
  data = dm_full,
  nrounds = best_params[["nrounds"]],
  verbose = FALSE
)

# Get probability predictions (z) for ALL data
z <- predict(final_model, xgb.DMatrix(data = xs))
```


```{r}
# Calculate residuals for XGBoost predictions
y <- as.numeric(as.character(select_data$attempt))
residuals <- y - z

# Histogram of residuals
hist(residuals, main="Distribution of XGBoost Model Residuals", xlab="Residual", breaks=100)
```

```{r}
# plot out the z values\
plot(density(z), 
     main = "Density of Z Predictions",
     xlab = "Predicted Probability of Attempt",
     ylab = "Density")
```

```{r}
# Calculate GIMR (lambda) This is Generalized inverse mills ratio
GIMR <- dnorm(qnorm(z)) / (1 - pnorm(qnorm(z)))

# Add GIMR to the data
select_data[, GIMR := GIMR]

# Print variable importa
```

```{r}
# Create a data.table with just my_id and GIMR from select_data
GIMR_dt <- data.table(my_id = select_data$my_id, GIMR = GIMR)

# Add GIMR to outcome_data by matching on my_id
outcome_data[GIMR_dt, GIMR := i.GIMR, on = "my_id"]
```

```{r}
# # Existing code
# outcome_data <- outcome_data[, my_id := NULL]
# outcome_data[, yardline_31_40 := NULL]
# outcome_data[, year2022_team_CLE := NULL] # tied for most attempts with 31

# Remove all columns with "_present" in the name
present_cols <- grep("_present", names(outcome_data), value = TRUE)
for (col in present_cols) {
  outcome_data[, (col) := NULL]
}
```


```{r}

# Create copies of outcome_data for different models
outcome_data_player <- copy(outcome_data)
outcome_data_starter <- copy(outcome_data)

# Remove specific columns as in original code
for (dt in list(outcome_data_player, outcome_data_starter)) {
  dt[, my_id := NULL]
  dt[, yardline_31_40 := NULL]
  dt[, year2022_team_CLE := NULL] # tied for most attempts with 31
  
  # Remove attendance variables
  dt[, attendance_pct := NULL]
  dt[, attendance_raw := NULL]
  dt[, vegas_wp_posteam := NULL]
}

# For player models, remove starter columns
player_cols <- grep("^starter_", names(outcome_data_player), value = TRUE)
for (col in player_cols) {
  outcome_data_player[, (col) := NULL]
}

# For starter models, remove player columns
starter_cols <- grep("^(offense|defense)_player_", names(outcome_data_starter), value = TRUE)
for (col in starter_cols) {
  outcome_data_starter[, (col) := NULL]
}

# Ensure conversion is numeric
for (dt in list(outcome_data_player, outcome_data_starter)) {
  dt[, conversion := as.numeric(as.character(conversion))]
}

# MODEL 1: Player model with GIMR
ols_model_player <- lm(conversion ~ ., data = outcome_data_player)

# MODEL 2: Starter model with GIMR
ols_model_starter <- lm(conversion ~ ., data = outcome_data_starter)

# Function to analyze and print model results
analyze_model <- function(model, model_name) {
  library(sandwich)
  library(lmtest)
  
  # Get HC0-adjusted t-values and p-values
  robust_test <- coeftest(model, vcov = vcovHC(model, type = "HC0"))
  t_stats <- robust_test[, "t value"]
  p_values <- robust_test[, "Pr(>|t|)"]
  
  # Function to add significance stars
  get_stars <- function(p) {
    if (p <= 0.01) return("***")
    if (p <= 0.05) return("**")
    if (p <= 0.1) return("*")
    if (p <= 0.15) return(".")
    return("")
  }
  
  # Maximum width for variable names
  name_width <- 60
  
  # Print model name and header
  cat("\n\n", paste(rep("=", 80), collapse=""), "\n")
  cat("MODEL:", model_name, "\n")
  cat(paste(rep("=", 80), collapse=""), "\n\n")
  
  # Print header with alignment
  cat(sprintf("%-*s  %10s  %10s  %4s\n", name_width, "Variable", "t-value", "p-value", "sig"))
  cat(sprintf("%-*s  %10s  %10s  %4s\n", name_width, "---------", "-------", "-------", "----"))
  
  # Loop through and print values
  for (i in 1:length(t_stats)) {
    var_name <- rownames(robust_test)[i]
    # Truncate name if too long
    if (nchar(var_name) > name_width) {
      var_name <- paste0(substr(var_name, 1, name_width-3), "...")
    }
    cat(sprintf("%-*s  %10.4f  %10.4f  %4s\n", 
                name_width, var_name, 
                t_stats[i], p_values[i],
                get_stars(p_values[i])))
  }
  
  cat("\nSignificance codes: 0 '***' 0.01 '**' 0.05 '*' 0.1 '.' 0.15\n")
  
  # Print R-squared
  r_squared <- summary(model)$adj.r.squared
  cat("\nAdjusted R-squared:", r_squared, "\n")
  
  # Calculate VIFs
  library(car)
  vifs <- vif(model)
  
  # Filter to show only VIFs over 5
  high_vifs <- vifs[vifs > 5]
  if(length(high_vifs) > 0) {
    cat("\nVariables with VIF > 5:\n")
    print(high_vifs)
  } else {
    cat("\nNo variables with VIF > 5 found in this model\n")
  }
  
  return(r_squared)
}

# Analyze models with GIMR only
r2_player <- analyze_model(ols_model_player, "Player Model with GIMR")
r2_starter <- analyze_model(ols_model_starter, "Starter Model with GIMR")

# Compare R-squared values
cat("\n\n", paste(rep("=", 80), collapse=""), "\n")
cat("MODEL COMPARISON - Adjusted R-squared values\n")
cat(paste(rep("=", 80), collapse=""), "\n\n")
cat("Player Model with GIMR:", r2_player, "\n")
cat("Starter Model with GIMR:", r2_starter, "\n")

```

